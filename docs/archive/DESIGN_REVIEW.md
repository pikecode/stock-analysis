# è‚¡ç¥¨æ¦‚å¿µåˆ†æç³»ç»Ÿ - è®¾è®¡è¯„å®¡æ€»ç»“

**è¯„å®¡æ—¥æœŸ**: 2025-11-22
**è¯„å®¡äºº**: Claude
**è®¾è®¡æ–‡æ¡£ç‰ˆæœ¬**: v1.0

---

## ğŸ“‹ è¯„å®¡ç»“æœæ¦‚è§ˆ

| è¯„å®¡é¡¹ | çŠ¶æ€ | è¯„åˆ† | è¯´æ˜ |
|--------|------|------|------|
| **æ•´ä½“æ¶æ„** | âœ… é€šè¿‡ | 9/10 | åˆ†å±‚æ¸…æ™°ï¼ŒæŠ€æœ¯é€‰å‹åˆç† |
| **æ•°æ®åº“è®¾è®¡** | âš ï¸ éœ€æ”¹è¿› | 7/10 | éƒ¨åˆ†è¡¨è®¾è®¡éœ€ä¼˜åŒ– |
| **APIè®¾è®¡** | âœ… é€šè¿‡ | 8/10 | RESTfulè§„èŒƒï¼Œè¦†ç›–å…¨é¢ |
| **å®‰å…¨è®¾è®¡** | âœ… é€šè¿‡ | 8/10 | è®¤è¯æˆæƒå®Œå–„ |
| **æ€§èƒ½æ–¹æ¡ˆ** | âš ï¸ éœ€æ”¹è¿› | 7/10 | ç¼“å­˜ç­–ç•¥éœ€ç»†åŒ– |
| **å¼€å‘è®¡åˆ’** | âœ… é€šè¿‡ | 8/10 | é˜¶æ®µæ¸…æ™°ï¼Œæ—¶é—´åˆç† |

**æ€»ä½“è¯„ä»·**: è®¾è®¡æ–¹æ¡ˆæ•´ä½“åˆç†ï¼ŒæŠ€æœ¯é€‰å‹ç¬¦åˆéœ€æ±‚ï¼Œä½†åœ¨æ•°æ®ä¸€è‡´æ€§ã€æ€§èƒ½ä¼˜åŒ–ã€ç›‘æ§å‘Šè­¦ç­‰æ–¹é¢éœ€è¦è¿›ä¸€æ­¥å®Œå–„ã€‚

---

## ğŸ”´ å…³é”®é—®é¢˜ (Must Fix)

### 1. æ•°æ®ä¸€è‡´æ€§é—®é¢˜

**é—®é¢˜æè¿°**:
CSVæ–‡ä»¶ä¸­çš„æ¦‚å¿µåç§°å¯èƒ½ä¸æ•°æ®åº“ä¸­çš„æ¦‚å¿µåç§°ä¸ä¸€è‡´ï¼Œä¾‹å¦‚ï¼š
- CSV: "äººå·¥æ™ºèƒ½"
- æ•°æ®åº“: "AI" æˆ– "äººå·¥æ™ºèƒ½æŠ€æœ¯"

**å½±å“**:
- å¯¼å…¥åäº§ç”Ÿé‡å¤æ¦‚å¿µ
- åŒä¸€æ¦‚å¿µè¢«æ‹†åˆ†æˆå¤šä¸ª
- æŸ¥è¯¢ç»“æœä¸å‡†ç¡®

**è§£å†³æ–¹æ¡ˆ**:

```sql
-- æ–°å¢æ¦‚å¿µæ˜ å°„è¡¨
CREATE TABLE concept_mappings (
    id SERIAL PRIMARY KEY,
    source_name VARCHAR(100) NOT NULL,     -- CSVä¸­çš„åç§°
    standard_name VARCHAR(100) NOT NULL,    -- æ ‡å‡†åŒ–åç§°
    concept_id INTEGER REFERENCES concepts(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(source_name)
);

-- ç¤ºä¾‹æ•°æ®
INSERT INTO concept_mappings (source_name, standard_name, concept_id) VALUES
('äººå·¥æ™ºèƒ½', 'äººå·¥æ™ºèƒ½', 1),
('AI', 'äººå·¥æ™ºèƒ½', 1),
('äººå·¥æ™ºèƒ½æŠ€æœ¯', 'äººå·¥æ™ºèƒ½', 1);
```

**å®ç°å»ºè®®**:
1. å¯¼å…¥æ—¶å…ˆæŸ¥è¯¢æ˜ å°„è¡¨
2. æœªæ‰¾åˆ°çš„åç§°æç¤ºç”¨æˆ·é€‰æ‹©æˆ–åˆ›å»º
3. æä¾›æ¦‚å¿µç®¡ç†é¡µé¢ï¼Œæ”¯æŒåˆå¹¶é‡å¤æ¦‚å¿µ
4. æ”¯æŒæ‰¹é‡å¯¼å…¥æ¦‚å¿µæ˜ å°„è§„åˆ™

---

### 2. æ•°æ®æ ¡éªŒä¸è¶³

**é—®é¢˜æè¿°**:
å½“å‰è®¾è®¡ä¸­æ•°æ®æ ¡éªŒè§„åˆ™ä¸å¤Ÿè¯¦ç»†ï¼Œå¯èƒ½å¯¼å…¥æ— æ•ˆæ•°æ®ã€‚

**ç¼ºå¤±çš„æ ¡éªŒ**:
- è‚¡ç¥¨ä»£ç æ ¼å¼éªŒè¯ (6ä½æ•°å­— æˆ– ç‰¹å®šå¯è½¬å€ºæ ¼å¼)
- æ—¥æœŸæ ¼å¼éªŒè¯
- äº¤æ˜“æ•°æ®åˆç†æ€§ (è´Ÿæ•°ã€å¼‚å¸¸å¤§å€¼)
- é‡å¤æ•°æ®æ£€æµ‹

**è§£å†³æ–¹æ¡ˆ**:

```python
# æ•°æ®æ ¡éªŒSchema
class StockDailyDataSchema(BaseModel):
    stock_code: str = Field(regex=r'^(\d{6}|1[12]\d{4})$')
    trade_date: date = Field(ge=date(2000, 1, 1), le=date.today())
    trade_value: int = Field(ge=0, le=10**12)

    @validator('stock_code')
    def validate_stock_code(cls, v):
        # æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦å­˜åœ¨
        if not stock_exists(v):
            raise ValueError(f'Stock {v} not found')
        return v

    @validator('trade_date')
    def validate_date_not_future(cls, v):
        if v > date.today():
            raise ValueError('Date cannot be in the future')
        return v

# é‡å¤æ•°æ®æ£€æµ‹
class ImportService:
    def check_duplicates(self, df, table_name):
        """æ£€æµ‹é‡å¤æ•°æ®"""
        if table_name == 'stock_daily_data':
            existing = db.query(
                StockDailyData.stock_code,
                StockDailyData.trade_date
            ).filter(
                StockDailyData.trade_date.in_(df['trade_date'].unique())
            ).all()

            duplicates = df[
                df[['stock_code', 'trade_date']].apply(
                    tuple, axis=1
                ).isin(existing)
            ]

            return duplicates
```

**å®ç°å»ºè®®**:
1. å¯¼å…¥å‰è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒ
2. æä¾›æ ¡éªŒæŠ¥å‘Šï¼Œæ ‡è®°é”™è¯¯è¡Œ
3. æ”¯æŒ"å¿½ç•¥é”™è¯¯ç»§ç»­"æˆ–"å…¨éƒ¨å›æ»š"
4. è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—ä¾›ä¸‹è½½

---

### 3. æ’åè®¡ç®—é€»è¾‘ä¸æ¸…æ™°

**é—®é¢˜æè¿°**:
å½“å‰è®¾è®¡ä¸­æœªæ˜ç¡®ï¼š
- æ’åè®¡ç®—çš„æ—¶æœº (å®æ—¶ vs å®šæ—¶)
- ç›¸åŒäº¤æ˜“å€¼çš„æ’åå¤„ç† (å¹¶åˆ—æ’åï¼Ÿ)
- ç©ºå€¼/ç¼ºå¤±æ•°æ®çš„æ’åå¤„ç†

**å»ºè®®æ–¹æ¡ˆ**:

```python
# æ’åè®¡ç®—ç­–ç•¥
class RankCalculationStrategy(Enum):
    REALTIME = "realtime"      # å®æ—¶è®¡ç®— (æŸ¥è¯¢æ—¶)
    SCHEDULED = "scheduled"    # å®šæ—¶è®¡ç®— (æ¯æ—¥å‡Œæ™¨)
    HYBRID = "hybrid"          # æ··åˆ (çƒ­é—¨å®æ—¶ï¼Œå…¶ä»–å®šæ—¶)

# æ’åè®¡ç®—é€»è¾‘
def calculate_concept_ranks(concept_id, date):
    """
    è®¡ç®—æŒ‡å®šæ¦‚å¿µåœ¨æŒ‡å®šæ—¥æœŸçš„è‚¡ç¥¨æ’å

    æ’åè§„åˆ™:
    1. æŒ‰ trade_value é™åºæ’åˆ—
    2. ç›¸åŒå€¼ä½¿ç”¨å¹³å‡æ’å (SQL: RANK() vs DENSE_RANK())
    3. ç¼ºå¤±æ•°æ®æ’åœ¨æœ€å
    """
    sql = """
    WITH ranked AS (
        SELECT
            sdd.stock_code,
            sdd.trade_value,
            RANK() OVER (ORDER BY sdd.trade_value DESC NULLS LAST) as rank,
            PERCENT_RANK() OVER (ORDER BY sdd.trade_value DESC) as percentile
        FROM stock_daily_data sdd
        JOIN stock_concepts sc ON sdd.stock_code = sc.stock_code
        WHERE sc.concept_id = :concept_id
          AND sdd.trade_date = :date
          AND sdd.trade_value IS NOT NULL
    )
    INSERT INTO concept_stock_daily_rank
        (concept_id, stock_code, trade_date, trade_value, rank, percentile)
    SELECT :concept_id, stock_code, :date, trade_value, rank, percentile * 100
    FROM ranked
    ON CONFLICT (concept_id, stock_code, trade_date)
    DO UPDATE SET
        trade_value = EXCLUDED.trade_value,
        rank = EXCLUDED.rank,
        percentile = EXCLUDED.percentile;
    """
```

**å®ç°å»ºè®®**:
1. é»˜è®¤ä½¿ç”¨å®šæ—¶è®¡ç®— (æ¯æ—¥å‡Œæ™¨2ç‚¹)
2. æä¾›æ‰‹åŠ¨è§¦å‘è®¡ç®—çš„API
3. çƒ­é—¨æ¦‚å¿µä½¿ç”¨ç¼“å­˜ + å®šæ—¶åˆ·æ–°
4. è®°å½•è®¡ç®—æ—¥å¿—ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜

---

### 4. ç¼ºå°‘æ•°æ®ç‰ˆæœ¬ç®¡ç†

**é—®é¢˜æè¿°**:
å¦‚æœå¯¼å…¥é”™è¯¯æ•°æ®ï¼Œæ— æ³•å›æ»šï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡åæœã€‚

**è§£å†³æ–¹æ¡ˆ**:

```sql
-- æ•°æ®ç‰ˆæœ¬è¡¨
CREATE TABLE data_versions (
    id SERIAL PRIMARY KEY,
    version_number VARCHAR(50) UNIQUE NOT NULL,  -- v1.0.0, v1.0.1
    import_record_id INTEGER REFERENCES import_records(id),
    description TEXT,
    created_by INTEGER REFERENCES users(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_current BOOLEAN DEFAULT false
);

-- æ•°æ®å˜æ›´è®°å½•
CREATE TABLE data_changes (
    id BIGSERIAL PRIMARY KEY,
    version_id INTEGER REFERENCES data_versions(id),
    table_name VARCHAR(100),
    action VARCHAR(20),  -- INSERT, UPDATE, DELETE
    record_id BIGINT,
    old_data JSONB,
    new_data JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**å®ç°å»ºè®®**:
1. æ¯æ¬¡å¯¼å…¥è‡ªåŠ¨åˆ›å»ºç‰ˆæœ¬
2. æä¾›å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬çš„åŠŸèƒ½
3. è½¯åˆ é™¤ + ç‰ˆæœ¬æ ‡è®°ï¼Œè€ŒéçœŸåˆ é™¤
4. ä¿ç•™æœ€è¿‘10ä¸ªç‰ˆæœ¬çš„å¿«ç…§

---

## âš ï¸ é‡è¦å»ºè®® (Should Fix)

### 5. æ€§èƒ½ç›‘æ§ç¼ºå¤±

**é—®é¢˜**: æ— æ³•åŠæ—¶å‘ç°æ€§èƒ½ç“¶é¢ˆå’Œå¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**:

**é›†æˆPrometheus + Grafana**:
```python
# æ·»åŠ metricsæ”¶é›†
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
request_count = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint'])
request_duration = Histogram('api_request_duration_seconds', 'API request duration')
active_users = Gauge('active_users_total', 'Number of active users')

# ä¸­é—´ä»¶
@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time

    request_count.labels(method=request.method, endpoint=request.url.path).inc()
    request_duration.observe(duration)

    return response
```

**å…³é”®æŒ‡æ ‡**:
- APIå“åº”æ—¶é—´ (P50, P95, P99)
- QPS (æ¯ç§’è¯·æ±‚æ•°)
- æ•°æ®åº“è¿æ¥æ± ä½¿ç”¨ç‡
- ç¼“å­˜å‘½ä¸­ç‡
- æ…¢æŸ¥è¯¢ç»Ÿè®¡
- é”™è¯¯ç‡

**å‘Šè­¦è§„åˆ™**:
```yaml
# Prometheuså‘Šè­¦è§„åˆ™
groups:
  - name: stock_analysis_alerts
    rules:
      - alert: HighAPILatency
        expr: api_request_duration_seconds{quantile="0.95"} > 0.5
        for: 5m
        annotations:
          summary: "APIå“åº”æ…¢ (P95 > 500ms)"

      - alert: HighErrorRate
        expr: rate(api_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        annotations:
          summary: "é”™è¯¯ç‡è¿‡é«˜ (>5%)"
```

---

### 6. å¢é‡å¯¼å…¥ä¼˜åŒ–

**å½“å‰é—®é¢˜**:
- æ¯æ¬¡éƒ½å…¨é‡å¯¼å…¥ï¼Œæ•ˆç‡ä½
- æ— æ³•è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ
- é‡å¤æ•°æ®å¤„ç†ä¸æ˜ç¡®

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
class IncrementalImportService:
    def import_daily_data(self, file_path, mode='increment'):
        """
        å¢é‡å¯¼å…¥ä¼˜åŒ–

        mode:
        - increment: åªæ’å…¥æ–°æ•°æ®ï¼Œè·³è¿‡å·²å­˜åœ¨
        - update: æ›´æ–°å·²å­˜åœ¨ï¼Œæ’å…¥æ–°æ•°æ®
        - replace: åˆ é™¤å½“æ—¥æ•°æ®ï¼Œé‡æ–°å¯¼å…¥
        """
        df = pd.read_csv(file_path)

        # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ
        date_column = self.detect_date_column(df)
        dates = df[date_column].unique()

        for date in dates:
            date_df = df[df[date_column] == date]

            if mode == 'increment':
                # è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®
                existing = self.get_existing_records(date)
                new_df = date_df[~date_df['stock_code'].isin(existing)]
                self.bulk_insert(new_df)

            elif mode == 'update':
                # UPSERTæ“ä½œ
                self.bulk_upsert(date_df)

            elif mode == 'replace':
                # åˆ é™¤å½“æ—¥æ•°æ®ï¼Œé‡æ–°å¯¼å…¥
                self.delete_by_date(date)
                self.bulk_insert(date_df)

    def detect_date_column(self, df):
        """è‡ªåŠ¨æ£€æµ‹æ—¥æœŸåˆ—"""
        for col in df.columns:
            if 'date' in col.lower() or 'æ—¥æœŸ' in col:
                return col
        # å°è¯•è§£æç¬¬ä¸€è¡Œ
        for col in df.columns:
            try:
                pd.to_datetime(df[col].iloc[0])
                return col
            except:
                continue
        raise ValueError("Cannot detect date column")
```

---

### 7. ç¼“å­˜ç­–ç•¥ç»†åŒ–

**å½“å‰é—®é¢˜**: ç¼“å­˜ç­–ç•¥æè¿°è¿‡äºç¬¼ç»Ÿ

**è¯¦ç»†æ–¹æ¡ˆ**:

| æ•°æ®ç±»å‹ | ç¼“å­˜ä½ç½® | TTL | æ›´æ–°ç­–ç•¥ |
|---------|---------|-----|---------|
| ç”¨æˆ·ä¿¡æ¯ | Redis | 1å°æ—¶ | ç™»å½•æ—¶æ›´æ–° |
| è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ | Redis | 1å¤© | ä¸»åŠ¨åˆ·æ–° |
| æ¦‚å¿µåˆ—è¡¨ | Redis | 6å°æ—¶ | ä¸»åŠ¨åˆ·æ–° |
| å½“æ—¥æ’å | Redis | 5åˆ†é’Ÿ | å®šæ—¶åˆ·æ–° |
| å†å²æ’å | é¢„è®¡ç®—è¡¨ | æ°¸ä¹… | å®šæ—¶è®¡ç®— |
| æ¦œå•æ•°æ® | Redis | 5åˆ†é’Ÿ | å®šæ—¶åˆ·æ–° |

**ç¼“å­˜KEYè®¾è®¡**:
```python
# ç¼“å­˜KEYè§„èŒƒ
CACHE_KEYS = {
    'user': 'user:{user_id}',
    'stock': 'stock:{stock_code}',
    'stock_concepts': 'stock:{stock_code}:concepts',
    'concept': 'concept:{concept_id}',
    'concept_ranks': 'concept:{concept_id}:ranks:{date}',
    'concept_summary': 'concept:{concept_id}:summary:{date}',
    'ranking_board': 'ranking:{type}:{date}',  # type: hot/active
}

# ç¼“å­˜å¤±æ•ˆç­–ç•¥
class CacheManager:
    def invalidate_stock_cache(self, stock_code):
        """è‚¡ç¥¨æ•°æ®æ›´æ–°æ—¶ï¼Œå¤±æ•ˆç›¸å…³ç¼“å­˜"""
        keys_to_delete = [
            f'stock:{stock_code}',
            f'stock:{stock_code}:concepts',
            f'stock:{stock_code}:ranks:*',
        ]
        redis.delete(*keys_to_delete)

    def invalidate_concept_cache(self, concept_id, date):
        """æ¦‚å¿µæ’åæ›´æ–°æ—¶ï¼Œå¤±æ•ˆç›¸å…³ç¼“å­˜"""
        keys = [
            f'concept:{concept_id}:ranks:{date}',
            f'concept:{concept_id}:summary:{date}',
            f'ranking:*:{date}',  # æ¦œå•ä¹Ÿéœ€è¦åˆ·æ–°
        ]
        redis.delete(*keys)
```

**é¢„çƒ­ç­–ç•¥**:
```python
async def warm_up_cache():
    """ç³»ç»Ÿå¯åŠ¨æ—¶é¢„çƒ­ç¼“å­˜"""
    # 1. åŠ è½½çƒ­é—¨æ¦‚å¿µ
    hot_concepts = await get_hot_concepts(limit=50)
    for concept in hot_concepts:
        await cache_concept(concept)

    # 2. åŠ è½½æœ€æ–°æ¦œå•
    latest_date = await get_latest_trade_date()
    await cache_ranking_board('hot', latest_date)
    await cache_ranking_board('active', latest_date)

    # 3. åŠ è½½ç³»ç»Ÿé…ç½®
    await cache_system_config()
```

---

### 8. å®šæ—¶ä»»åŠ¡ç®¡ç†

**å½“å‰é—®é¢˜**: ç¼ºå°‘å®šæ—¶ä»»åŠ¡çš„é…ç½®å’Œç›‘æ§

**è§£å†³æ–¹æ¡ˆ**:

```python
# Celery Beaté…ç½®
from celery.schedules import crontab

app.conf.beat_schedule = {
    # æ¯æ—¥å‡Œæ™¨2ç‚¹è®¡ç®—æ’å
    'calculate-daily-ranks': {
        'task': 'app.tasks.calculate_daily_ranks',
        'schedule': crontab(hour=2, minute=0),
        'args': (date.today() - timedelta(days=1),)
    },

    # æ¯æ—¥å‡Œæ™¨3ç‚¹æ±‡æ€»æ¦‚å¿µæ•°æ®
    'summarize-concept-data': {
        'task': 'app.tasks.summarize_concept_data',
        'schedule': crontab(hour=3, minute=0),
    },

    # æ¯å°æ—¶åˆ·æ–°çƒ­é—¨æ•°æ®ç¼“å­˜
    'refresh-hot-data-cache': {
        'task': 'app.tasks.refresh_hot_data_cache',
        'schedule': crontab(minute=0),
    },

    # æ¯å¤©å‡Œæ™¨4ç‚¹å¤‡ä»½æ•°æ®åº“
    'backup-database': {
        'task': 'app.tasks.backup_database',
        'schedule': crontab(hour=4, minute=0),
    },

    # æ¯å‘¨æ—¥å‡Œæ™¨æ¸…ç†è¿‡æœŸæ—¥å¿—
    'cleanup-old-logs': {
        'task': 'app.tasks.cleanup_old_logs',
        'schedule': crontab(day_of_week=0, hour=5, minute=0),
    },
}

# ä»»åŠ¡ç›‘æ§
class TaskMonitor:
    def record_task_execution(self, task_name, status, duration, error=None):
        """è®°å½•ä»»åŠ¡æ‰§è¡Œæƒ…å†µ"""
        TaskExecutionLog.create(
            task_name=task_name,
            status=status,  # success, failed, timeout
            duration=duration,
            error_message=error,
            executed_at=datetime.now()
        )

        # å¤±è´¥å‘Šè­¦
        if status == 'failed':
            self.send_alert(
                f"å®šæ—¶ä»»åŠ¡å¤±è´¥: {task_name}",
                f"é”™è¯¯ä¿¡æ¯: {error}"
            )
```

**ä»»åŠ¡æ‰§è¡Œæ—¥å¿—è¡¨**:
```sql
CREATE TABLE task_execution_logs (
    id BIGSERIAL PRIMARY KEY,
    task_name VARCHAR(100) NOT NULL,
    status VARCHAR(20) NOT NULL,
    duration FLOAT,
    error_message TEXT,
    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_task_logs_name ON task_execution_logs(task_name);
CREATE INDEX idx_task_logs_status ON task_execution_logs(status);
```

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®® (Nice to Have)

### 9. æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–

**å»ºè®®é…ç½®**:
```python
# SQLAlchemyè¿æ¥æ± é…ç½®
engine = create_engine(
    DATABASE_URL,
    pool_size=20,           # å¸¸é©»è¿æ¥æ•°
    max_overflow=10,        # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
    pool_timeout=30,        # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
    pool_recycle=3600,      # è¿æ¥å›æ”¶æ—¶é—´
    pool_pre_ping=True,     # è¿æ¥æ£€æµ‹
    echo_pool=True,         # è¿æ¥æ± æ—¥å¿—
)

# æ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´
# è½»è´Ÿè½½: pool_size=10, max_overflow=5
# ä¸­è´Ÿè½½: pool_size=20, max_overflow=10
# é‡è´Ÿè½½: pool_size=50, max_overflow=20
```

---

### 10. APIé™æµç»†åŒ–

**å»ºè®®æ–¹æ¡ˆ**:

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

# ä¸åŒç«¯ç‚¹ä¸åŒé™æµç­–ç•¥
@app.get("/api/v1/stocks")
@limiter.limit("100/minute")  # æŸ¥è¯¢æ¥å£
async def get_stocks():
    pass

@app.post("/api/v1/import/execute")
@limiter.limit("10/hour")  # å¯¼å…¥æ¥å£
async def execute_import():
    pass

@app.post("/api/v1/auth/login")
@limiter.limit("5/minute")  # ç™»å½•æ¥å£
async def login():
    pass

# ä¸åŒç”¨æˆ·è§’è‰²ä¸åŒé™æµ
class RoleBasedRateLimiter:
    LIMITS = {
        'super_admin': "1000/minute",
        'admin': "500/minute",
        'analyst': "200/minute",
        'viewer': "100/minute",
    }

    async def check_limit(self, user):
        role = user.role
        limit = self.LIMITS[role]
        # æ£€æŸ¥æ˜¯å¦è¶…é™
```

---

### 11. å‰ç«¯æ€§èƒ½ä¼˜åŒ–å»ºè®®

**è™šæ‹Ÿæ»šåŠ¨**:
```vue
<!-- å¤§åˆ—è¡¨ä½¿ç”¨è™šæ‹Ÿæ»šåŠ¨ -->
<template>
  <virtual-scroller
    :items="stockList"
    :item-height="50"
    :buffer="200"
  >
    <template #default="{ item }">
      <stock-item :stock="item" />
    </template>
  </virtual-scroller>
</template>
```

**æ‡’åŠ è½½**:
```typescript
// è·¯ç”±æ‡’åŠ è½½
const routes = [
  {
    path: '/stock',
    component: () => import('@/views/stock/StockList.vue')
  }
]

// å›¾ç‰‡æ‡’åŠ è½½
<img v-lazy="imageUrl" />
```

**é˜²æŠ–èŠ‚æµ**:
```typescript
import { debounce } from 'lodash-es'

const handleSearch = debounce((keyword: string) => {
  searchStocks(keyword)
}, 300)
```

---

### 12. æµ‹è¯•ç­–ç•¥

**æµ‹è¯•é‡‘å­—å¡”**:
```
       /\
      /E2E\          10%  ç«¯åˆ°ç«¯æµ‹è¯•
     /------\
    /  é›†æˆ  \        20%  é›†æˆæµ‹è¯•
   /----------\
  /   å•å…ƒæµ‹è¯•  \     70%  å•å…ƒæµ‹è¯•
 /--------------\
```

**è¦†ç›–ç‡ç›®æ ‡**:
- å•å…ƒæµ‹è¯•: 70%
- é›†æˆæµ‹è¯•: ä¸»è¦ä¸šåŠ¡æµç¨‹
- E2Eæµ‹è¯•: å…³é”®ç”¨æˆ·è·¯å¾„

**æµ‹è¯•æ¸…å•**:

**åç«¯æµ‹è¯•**:
```python
# å•å…ƒæµ‹è¯•ç¤ºä¾‹
def test_calculate_ranks():
    """æµ‹è¯•æ’åè®¡ç®—é€»è¾‘"""
    concept = create_test_concept()
    stocks = create_test_stocks(10)

    ranks = calculate_concept_ranks(concept.id, date.today())

    assert len(ranks) == 10
    assert ranks[0].rank == 1
    assert ranks[0].trade_value > ranks[1].trade_value

# é›†æˆæµ‹è¯•ç¤ºä¾‹
async def test_import_workflow():
    """æµ‹è¯•å®Œæ•´å¯¼å…¥æµç¨‹"""
    file = upload_test_file()
    preview = await preview_data(file.path)
    assert len(preview) > 0

    task = await execute_import(file.path, mode='increment')
    await wait_for_task(task.id)

    result = await get_import_result(task.id)
    assert result.status == 'success'
```

**å‰ç«¯æµ‹è¯•**:
```typescript
// ç»„ä»¶æµ‹è¯•
import { mount } from '@vue/test-utils'
import StockList from '@/views/stock/StockList.vue'

describe('StockList', () => {
  it('renders stock list', async () => {
    const wrapper = mount(StockList)
    await wrapper.vm.$nextTick()

    expect(wrapper.find('.stock-item').exists()).toBe(true)
  })

  it('searches stocks', async () => {
    const wrapper = mount(StockList)
    await wrapper.find('input').setValue('600000')
    await wrapper.find('button').trigger('click')

    expect(wrapper.vm.stockList).toHaveLength(1)
  })
})
```

---

## ğŸ“Š æ•°æ®åº“è®¾è®¡è¡¥å……å»ºè®®

### ç¼ºå¤±çš„ç´¢å¼•

```sql
-- stock_daily_data éœ€è¦çš„å¤åˆç´¢å¼•
CREATE INDEX idx_stock_daily_stock_date_value
    ON stock_daily_data(stock_code, trade_date, trade_value DESC);

-- ç”¨äºå¿«é€ŸæŸ¥æ‰¾æŸè‚¡ç¥¨æŸæ—¥æœŸçš„æ•°æ®
CREATE INDEX idx_stock_daily_date_stock
    ON stock_daily_data(trade_date, stock_code);

-- concept_stock_daily_rank è¦†ç›–ç´¢å¼•
CREATE INDEX idx_concept_rank_cover
    ON concept_stock_daily_rank(concept_id, trade_date, rank)
    INCLUDE (stock_code, trade_value);

-- audit_logs æ—¶é—´èŒƒå›´æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX idx_audit_logs_date_range
    ON audit_logs(created_at DESC, user_id);
```

### åˆ†åŒºè¡¨è¡¥å……

```sql
-- è‡ªåŠ¨åˆ›å»ºæœªæ¥åˆ†åŒºçš„å‡½æ•°
CREATE OR REPLACE FUNCTION create_partition_if_not_exists(
    table_name TEXT,
    partition_date DATE
) RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
    start_date DATE;
    end_date DATE;
BEGIN
    partition_name := table_name || '_' || to_char(partition_date, 'YYYY_MM');
    start_date := date_trunc('month', partition_date);
    end_date := start_date + INTERVAL '1 month';

    IF NOT EXISTS (
        SELECT 1 FROM pg_class WHERE relname = partition_name
    ) THEN
        EXECUTE format(
            'CREATE TABLE %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
            partition_name, table_name, start_date, end_date
        );
    END IF;
END;
$$ LANGUAGE plpgsql;

-- å®šæ—¶ä»»åŠ¡ï¼šæå‰åˆ›å»ºä¸‹ä¸ªæœˆçš„åˆ†åŒº
-- Cron: æ¯æœˆ25å·åˆ›å»ºä¸‹æœˆåˆ†åŒº
```

---

## ğŸ¯ å¼€å‘ä¼˜å…ˆçº§è°ƒæ•´å»ºè®®

åŸºäºä¸Šè¿°é—®é¢˜ï¼Œå»ºè®®è°ƒæ•´å¼€å‘è®¡åˆ’ï¼š

### è°ƒæ•´åçš„Phaseåˆ’åˆ†

**Phase 1: åŸºç¡€æ¡†æ¶ + æ•°æ®è§„èŒƒåŒ– (1.5å‘¨)**
- åŸPhase 1å†…å®¹
- âœ¨ **æ–°å¢**: æ¦‚å¿µæ˜ å°„è¡¨è®¾è®¡å’Œå®ç°
- âœ¨ **æ–°å¢**: æ•°æ®æ ¡éªŒè§„åˆ™å®Œå–„

**Phase 2: æ•°æ®å¯¼å…¥ + ç‰ˆæœ¬ç®¡ç† (1.5å‘¨)**
- åŸPhase 2å†…å®¹
- âœ¨ **æ–°å¢**: æ•°æ®ç‰ˆæœ¬ç®¡ç†
- âœ¨ **æ–°å¢**: å¢é‡å¯¼å…¥ä¼˜åŒ–
- âœ¨ **æ–°å¢**: è¯¦ç»†çš„æ ¡éªŒæŠ¥å‘Š

**Phase 3: æ ¸å¿ƒæŸ¥è¯¢ + æ’åè®¡ç®— (1å‘¨)**
- åŸPhase 3å†…å®¹
- âœ¨ **æ–°å¢**: æ˜ç¡®æ’åè®¡ç®—é€»è¾‘
- âœ¨ **æ–°å¢**: å®šæ—¶ä»»åŠ¡è°ƒåº¦

**Phase 4: å¯è§†åŒ– + ç¼“å­˜ä¼˜åŒ– (1å‘¨)**
- åŸPhase 4å†…å®¹
- âœ¨ **æ–°å¢**: ç»†åŒ–ç¼“å­˜ç­–ç•¥
- âœ¨ **æ–°å¢**: ç¼“å­˜é¢„çƒ­å’Œå¤±æ•ˆæœºåˆ¶

**Phase 5: ç”¨æˆ·ç®¡ç† + ç›‘æ§ (1å‘¨)**
- åŸPhase 5å†…å®¹
- âœ¨ **æ–°å¢**: Prometheusé›†æˆ
- âœ¨ **æ–°å¢**: å…³é”®æŒ‡æ ‡ç›‘æ§
- âœ¨ **æ–°å¢**: å‘Šè­¦è§„åˆ™é…ç½®

**Phase 6: é«˜çº§åŠŸèƒ½ + å®Œå–„ (3å¤©)**
- åŸPhase 6å†…å®¹
- âœ¨ **æ–°å¢**: æ¦‚å¿µç®¡ç†é¡µé¢
- âœ¨ **æ–°å¢**: æ•°æ®ç‰ˆæœ¬å›æ»š

**Phase 7: æµ‹è¯•ä¼˜åŒ– (1å‘¨)**
- åŸPhase 7å†…å®¹
- âœ¨ **æ–°å¢**: å•å…ƒæµ‹è¯•å®Œå–„
- âœ¨ **æ–°å¢**: æ€§èƒ½å‹æµ‹
- âœ¨ **æ–°å¢**: å®‰å…¨æµ‹è¯•

**Phase 8: éƒ¨ç½²ä¸Šçº¿ (2å¤©)**
- åŸPhase 8å†…å®¹

**æ–°çš„æ€»æ—¶é•¿**: çº¦5.5å‘¨

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹å¼€å‘å‰ï¼Œè¯·ç¡®è®¤ä»¥ä¸‹äº‹é¡¹ï¼š

### éœ€æ±‚ç¡®è®¤
- [ ] ç¡®è®¤6å¤§åˆ†æéœ€æ±‚çš„è¯¦ç»†é€»è¾‘
- [ ] ç¡®è®¤æ¦‚å¿µå˜æ›´æ˜¯å¦éœ€è¦å†å²è®°å½•
- [ ] ç¡®è®¤æ•°æ®ä¿ç•™ç­–ç•¥ (å…¨éƒ¨ä¿ç•™ vs å®šæœŸå½’æ¡£)
- [ ] ç¡®è®¤æ˜¯å¦éœ€è¦å¤šç§Ÿæˆ·æ”¯æŒ

### æŠ€æœ¯ç¡®è®¤
- [ ] PostgreSQLç‰ˆæœ¬ç¡®è®¤ (å»ºè®®15+)
- [ ] Redisç‰ˆæœ¬ç¡®è®¤ (å»ºè®®7+)
- [ ] æœåŠ¡å™¨é…ç½®ç¡®è®¤ (CPU/å†…å­˜/ç£ç›˜)
- [ ] åŸŸåå’ŒSSLè¯ä¹¦å‡†å¤‡

### æ•°æ®ç¡®è®¤
- [ ] CSVæ•°æ®æ ¼å¼æ ‡å‡†åŒ–
- [ ] æ¦‚å¿µåç§°ç»Ÿä¸€è§„èŒƒ
- [ ] å†å²æ•°æ®å¯¼å…¥è®¡åˆ’
- [ ] æ•°æ®è´¨é‡æ£€æŸ¥

### å›¢é˜Ÿç¡®è®¤
- [ ] å¼€å‘äººå‘˜æŠ€æœ¯æ ˆç†Ÿæ‚‰åº¦
- [ ] æµ‹è¯•èµ„æºå®‰æ’
- [ ] è¿ç»´æ”¯æŒç¡®è®¤
- [ ] é¡¹ç›®æ—¶é—´è¡¨ç¡®è®¤

---

## ğŸ“ åç»­è¡ŒåŠ¨å»ºè®®

### ç«‹å³è¡ŒåŠ¨ (æœ¬å‘¨)
1. âœ… ä¸å›¢é˜Ÿè¯„å®¡æ­¤è®¾è®¡æ–‡æ¡£
2. âœ… ç¡®è®¤å¹¶è§£ç­”æ‰€æœ‰å¾…ç¡®è®¤çš„è®¾è®¡å†³ç­–
3. âœ… è¡¥å……å®Œå–„å…³é”®é—®é¢˜çš„è®¾è®¡æ–¹æ¡ˆ
4. âœ… å‡†å¤‡å¼€å‘ç¯å¢ƒ

### çŸ­æœŸè¡ŒåŠ¨ (ä¸‹å‘¨)
1. ğŸ“ æ­å»ºé¡¹ç›®æ¡†æ¶
2. ğŸ—„ï¸ åˆ›å»ºæ•°æ®åº“å¹¶åˆå§‹åŒ–
3. ğŸ§ª å®ç°æ¦‚å¿µæ˜ å°„åŠŸèƒ½
4. ğŸ“Š å®ç°æ•°æ®æ ¡éªŒè§„åˆ™

### ä¸­æœŸè¡ŒåŠ¨ (2-4å‘¨)
1. ğŸ”§ æŒ‰è°ƒæ•´åçš„Phaseè®¡åˆ’å¼€å‘
2. ğŸ“ˆ æŒç»­é›†æˆæµ‹è¯•
3. ğŸ¯ æ¯å‘¨è¿›è¡Œè¿›åº¦è¯„å®¡
4. ğŸ› åŠæ—¶ä¿®å¤å‘ç°çš„é—®é¢˜

---

## ğŸ’­ æœ€ç»ˆå»ºè®®

è¿™æ˜¯ä¸€ä¸ª**è®¾è®¡åˆç†ã€ç›®æ ‡æ˜ç¡®**çš„ç³»ç»Ÿï¼Œä¸»è¦ä¼˜ç‚¹ï¼š
- âœ… æŠ€æœ¯æ ˆç°ä»£ä¸”æˆç†Ÿ
- âœ… æ¶æ„æ¸…æ™°æ˜“æ‰©å±•
- âœ… åŠŸèƒ½è¦†ç›–å®Œæ•´

éœ€è¦é‡ç‚¹å…³æ³¨ï¼š
- ğŸ”´ æ•°æ®ä¸€è‡´æ€§ä¿è¯
- ğŸ”´ æ•°æ®æ ¡éªŒå®Œå–„
- ğŸ”´ æ€§èƒ½ç›‘æ§å»ºç«‹
- âš ï¸ ç¼“å­˜ç­–ç•¥ç»†åŒ–
- âš ï¸ æµ‹è¯•è¦†ç›–ç‡

å»ºè®®ï¼š
1. **å…ˆå°æ­¥å¿«è·‘**: å¿«é€Ÿå®ç°MVPï¼ŒéªŒè¯æ ¸å¿ƒåŠŸèƒ½
2. **æŒç»­è¿­ä»£**: æ ¹æ®ä½¿ç”¨åé¦ˆä¸æ–­ä¼˜åŒ–
3. **é‡è§†æµ‹è¯•**: å°½æ—©å»ºç«‹è‡ªåŠ¨åŒ–æµ‹è¯•
4. **ç›‘æ§å…ˆè¡Œ**: ä¸Šçº¿å‰å¿…é¡»æœ‰ç›‘æ§å‘Šè­¦

**é¢„æœŸæ•ˆæœ**:
æŒ‰ç…§è°ƒæ•´åçš„è®¡åˆ’ï¼Œçº¦5.5å‘¨å¯ä»¥å®Œæˆä¸€ä¸ª**åŠŸèƒ½å®Œæ•´ã€æ€§èƒ½è‰¯å¥½ã€ç›‘æ§å®Œå–„**çš„MVPç‰ˆæœ¬ã€‚

---

**è¯„å®¡çŠ¶æ€**: âœ… è¯„å®¡å®Œæˆ
**å»ºè®®**: æ ¹æ®æœ¬æ–‡æ¡£ä¿®è®¢è®¾è®¡æ–¹æ¡ˆåå¼€å§‹å¼€å‘
**ä¸‹æ¬¡è¯„å®¡**: ç¬¬ä¸€ä¸ªPhaseå®Œæˆå
