#!/usr/bin/env python
"""
æ‰¹é‡å¯¼å…¥è„šæœ¬ - æ”¯æŒå¤§æ–‡ä»¶è‡ªåŠ¨æ‹†åˆ†å’Œå¹¶è¡Œå¤„ç†
Usage:
    python scripts/batch_import.py <file> --type TXT --metric-code <code> [options]

Examples:
    # åŸºç¡€ç”¨æ³•
    python scripts/batch_import.py /path/to/EEE.txt --type TXT --metric-code EEE

    # å¹¶è¡Œå¤„ç†
    python scripts/batch_import.py /path/to/EEE.txt --type TXT --metric-code EEE --parallel 4

    # ä»æŒ‡å®šæ—¥æœŸç»§ç»­
    python scripts/batch_import.py /path/to/EEE.txt --type TXT --metric-code EEE --resume-from 2024-01-01

    # åªå¤„ç†ç‰¹å®šæ—¥æœŸèŒƒå›´
    python scripts/batch_import.py /path/to/EEE.txt --type TXT --metric-code EEE --start-date 2024-01-01 --end-date 2024-12-31
"""

import sys
import os
from pathlib import Path
from datetime import datetime, date
from typing import Dict, List, Tuple, Optional, Set
import argparse
import json
from dataclasses import dataclass, asdict
from collections import defaultdict
import multiprocessing as mp
from multiprocessing import Pool, Queue, Manager
import logging
from tqdm import tqdm
import hashlib
import tempfile
import signal
import atexit

# æ·»åŠ é¡¹ç›®è·¯å¾„
# è„šæœ¬ä½ç½®: scripts/imports/batch_import.py
# éœ€è¦è®¿é—®: backend/app/...
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
backend_path = os.path.join(project_root, 'backend')
sys.path.insert(0, backend_path)

from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
from app.core.config import settings
from app.services.optimized_txt_import import OptimizedTXTImportService
from app.models.stock import ImportBatch, MetricType

# åˆ›å»ºæ•°æ®åº“ä¼šè¯
engine = create_engine(settings.DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class DateBatch:
    """æ—¥æœŸæ‰¹æ¬¡æ•°æ®"""
    trade_date: str
    lines: List[str]
    count: int


@dataclass
class ImportProgress:
    """å¯¼å…¥è¿›åº¦è®°å½•"""
    file_path: str
    file_hash: str
    total_dates: int
    processed_dates: Set[str]
    failed_dates: Set[str]
    start_time: str
    last_update: str

    def to_dict(self):
        """è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„å­—å…¸"""
        return {
            'file_path': self.file_path,
            'file_hash': self.file_hash,
            'total_dates': self.total_dates,
            'processed_dates': list(self.processed_dates),
            'failed_dates': list(self.failed_dates),
            'start_time': self.start_time,
            'last_update': self.last_update
        }

    @classmethod
    def from_dict(cls, data: dict):
        """ä»å­—å…¸æ¢å¤"""
        return cls(
            file_path=data['file_path'],
            file_hash=data['file_hash'],
            total_dates=data['total_dates'],
            processed_dates=set(data.get('processed_dates', [])),
            failed_dates=set(data.get('failed_dates', [])),
            start_time=data['start_time'],
            last_update=data['last_update']
        )


class BatchImporter:
    """æ‰¹é‡å¯¼å…¥å™¨"""

    def __init__(self, metric_code: str, parallel: int = 1):
        self.metric_code = metric_code
        self.parallel = max(1, min(parallel, mp.cpu_count()))
        self.progress_file = f"/tmp/batch_import_{metric_code}.json"
        self.temp_dir = tempfile.mkdtemp(prefix=f"batch_import_{metric_code}_")
        self.progress = None

        # æ³¨å†Œæ¸…ç†å‡½æ•°
        atexit.register(self.cleanup)
        signal.signal(signal.SIGINT, self._handle_interrupt)

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if os.path.exists(self.temp_dir):
            import shutil
            shutil.rmtree(self.temp_dir, ignore_errors=True)

    def _handle_interrupt(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œä¿å­˜è¿›åº¦...")
        if self.progress:
            self.save_progress()
        self.cleanup()
        sys.exit(0)

    def get_file_hash(self, file_path: str, sample_size: int = 1024*1024) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œï¼ˆåŸºäºæ–‡ä»¶å¤´éƒ¨é‡‡æ ·ï¼‰"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            hasher.update(f.read(sample_size))
        return hasher.hexdigest()

    def load_progress(self, file_path: str) -> Optional[ImportProgress]:
        """åŠ è½½è¿›åº¦è®°å½•"""
        if not os.path.exists(self.progress_file):
            return None

        try:
            with open(self.progress_file, 'r') as f:
                data = json.load(f)
            progress = ImportProgress.from_dict(data)

            # éªŒè¯æ–‡ä»¶å“ˆå¸Œ
            current_hash = self.get_file_hash(file_path)
            if progress.file_hash != current_hash:
                logger.warning("æ–‡ä»¶å·²å˜æ›´ï¼Œé‡æ–°å¼€å§‹å¯¼å…¥")
                return None

            return progress
        except Exception as e:
            logger.error(f"åŠ è½½è¿›åº¦å¤±è´¥: {e}")
            return None

    def save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        if not self.progress:
            return

        try:
            self.progress.last_update = datetime.now().isoformat()
            with open(self.progress_file, 'w') as f:
                json.dump(self.progress.to_dict(), f, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

    def scan_file(self, file_path: str) -> Dict[str, DateBatch]:
        """æ‰«ææ–‡ä»¶ï¼ŒæŒ‰æ—¥æœŸåˆ†ç»„"""
        logger.info(f"å¼€å§‹æ‰«ææ–‡ä»¶: {file_path}")

        date_batches = defaultdict(list)
        line_count = 0

        # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312']
        content = None

        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.readlines()
                break
            except UnicodeDecodeError:
                continue

        if content is None:
            raise ValueError(f"æ— æ³•è§£ææ–‡ä»¶ç¼–ç : {file_path}")

        # åˆ†ç»„æ•°æ®
        with tqdm(total=len(content), desc="æ‰«ææ–‡ä»¶") as pbar:
            for line in content:
                line = line.strip()
                if not line:
                    continue

                parts = line.split('\t') if '\t' in line else line.split()
                if len(parts) >= 3:
                    trade_date = parts[1]
                    date_batches[trade_date].append(line)
                    line_count += 1

                pbar.update(1)

        # è½¬æ¢ä¸ºDateBatchå¯¹è±¡
        result = {}
        for trade_date, lines in date_batches.items():
            result[trade_date] = DateBatch(
                trade_date=trade_date,
                lines=lines,
                count=len(lines)
            )

        logger.info(f"æ‰«æå®Œæˆ: {len(result)}ä¸ªæ—¥æœŸ, {line_count}æ¡æ•°æ®")
        return result

    def save_date_batch(self, date_batch: DateBatch) -> str:
        """ä¿å­˜å•ä¸ªæ—¥æœŸæ‰¹æ¬¡åˆ°ä¸´æ—¶æ–‡ä»¶"""
        temp_file = os.path.join(self.temp_dir, f"{self.metric_code}_{date_batch.trade_date}.txt")

        with open(temp_file, 'w', encoding='utf-8') as f:
            for line in date_batch.lines:
                f.write(line + '\n')

        return temp_file

    def import_single_date(self, args: Tuple[str, str, str, int]) -> Tuple[str, bool, str]:
        """å¯¼å…¥å•ä¸ªæ—¥æœŸçš„æ•°æ®ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰"""
        trade_date_str, temp_file, metric_code, metric_type_id = args

        try:
            # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯
            db = SessionLocal()

            # è§£ææ—¥æœŸ
            trade_date = datetime.strptime(trade_date_str, '%Y-%m-%d').date()

            # è¯»å–ä¸´æ—¶æ–‡ä»¶
            with open(temp_file, 'rb') as f:
                file_content = f.read()

            # åˆ›å»ºå¯¼å…¥æ‰¹æ¬¡
            import_batch = ImportBatch(
                file_name=f"{metric_code}_{trade_date_str}.txt",
                file_type='TXT',
                file_size=len(file_content),
                file_hash=hashlib.md5(file_content).hexdigest(),
                total_rows=0,
                success_rows=0,
                error_rows=0,
                status='processing',
                created_by=1
            )
            db.add(import_batch)
            db.commit()

            # æ‰§è¡Œå¯¼å…¥
            service = OptimizedTXTImportService(db)
            success_count, error_count = service.parse_and_import_with_compute(
                batch_id=import_batch.id,
                file_content=file_content,
                metric_type_id=metric_type_id,
                metric_code=metric_code,
                data_date=trade_date
            )

            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
            import_batch.status = 'completed'
            import_batch.success_rows = success_count
            import_batch.error_rows = error_count
            import_batch.total_rows = success_count + error_count
            import_batch.completed_at = datetime.now()
            db.commit()

            db.close()

            return trade_date_str, True, f"æˆåŠŸ: {success_count}æ¡"

        except Exception as e:
            logger.error(f"å¯¼å…¥å¤±è´¥ {trade_date_str}: {str(e)}")
            return trade_date_str, False, f"å¤±è´¥: {str(e)}"

    def run_parallel_import(
        self,
        date_batches: Dict[str, DateBatch],
        metric_type_id: int,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ):
        """å¹¶è¡Œæ‰§è¡Œå¯¼å…¥"""

        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        dates_to_process = []
        for trade_date, batch in date_batches.items():
            # è·³è¿‡å·²å¤„ç†çš„
            if self.progress and trade_date in self.progress.processed_dates:
                continue

            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            if start_date and trade_date < start_date:
                continue
            if end_date and trade_date > end_date:
                continue

            dates_to_process.append((trade_date, batch))

        if not dates_to_process:
            logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ—¥æœŸ")
            return

        # æ’åº
        dates_to_process.sort(key=lambda x: x[0])

        logger.info(f"å‡†å¤‡å¤„ç† {len(dates_to_process)} ä¸ªæ—¥æœŸï¼Œä½¿ç”¨ {self.parallel} ä¸ªè¿›ç¨‹")

        # å‡†å¤‡ä»»åŠ¡
        tasks = []
        for trade_date, batch in dates_to_process:
            temp_file = self.save_date_batch(batch)
            tasks.append((trade_date, temp_file, self.metric_code, metric_type_id))

        # åˆ›å»ºè¿›åº¦æ¡
        with tqdm(total=len(tasks), desc="å¯¼å…¥è¿›åº¦") as pbar:
            if self.parallel == 1:
                # å•è¿›ç¨‹
                for task in tasks:
                    trade_date, success, msg = self.import_single_date(task)

                    if success:
                        self.progress.processed_dates.add(trade_date)
                    else:
                        self.progress.failed_dates.add(trade_date)

                    pbar.set_postfix_str(f"{trade_date}: {msg}")
                    pbar.update(1)
                    self.save_progress()
            else:
                # å¤šè¿›ç¨‹
                with Pool(processes=self.parallel) as pool:
                    # ä½¿ç”¨imap_unorderedè·å–ç»“æœ
                    for trade_date, success, msg in pool.imap_unordered(
                        self.import_single_date, tasks
                    ):
                        if success:
                            self.progress.processed_dates.add(trade_date)
                        else:
                            self.progress.failed_dates.add(trade_date)

                        pbar.set_postfix_str(f"{trade_date}: {msg}")
                        pbar.update(1)
                        self.save_progress()

    def import_file(
        self,
        file_path: str,
        resume: bool = False,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ):
        """æ‰§è¡Œæ–‡ä»¶å¯¼å…¥"""

        # è·å–metric_type_id
        db = SessionLocal()
        metric_type = db.query(MetricType).filter_by(code=self.metric_code).first()
        if not metric_type:
            # åˆ›å»ºæ–°çš„æŒ‡æ ‡ç±»å‹
            metric_type = MetricType(
                code=self.metric_code,
                name=self.metric_code,
                description=f"{self.metric_code} äº¤æ˜“æ•°æ®"
            )
            db.add(metric_type)
            db.commit()

        metric_type_id = metric_type.id
        db.close()

        # åŠ è½½æˆ–åˆ›å»ºè¿›åº¦
        if resume:
            self.progress = self.load_progress(file_path)

        if not self.progress:
            self.progress = ImportProgress(
                file_path=file_path,
                file_hash=self.get_file_hash(file_path),
                total_dates=0,
                processed_dates=set(),
                failed_dates=set(),
                start_time=datetime.now().isoformat(),
                last_update=datetime.now().isoformat()
            )

        # æ‰«ææ–‡ä»¶
        date_batches = self.scan_file(file_path)
        self.progress.total_dates = len(date_batches)

        # æ˜¾ç¤ºç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
        print(f"  æ€»æ—¥æœŸæ•°: {len(date_batches)}")
        print(f"  æ€»æ•°æ®é‡: {sum(b.count for b in date_batches.values())}")
        print(f"  å·²å¤„ç†: {len(self.progress.processed_dates)}")
        print(f"  å¤±è´¥: {len(self.progress.failed_dates)}")

        if self.progress.processed_dates:
            print(f"  ç»§ç»­ä»: {min(self.progress.processed_dates)}")

        print("="*60 + "\n")

        # æ‰§è¡Œå¯¼å…¥
        start_time = datetime.now()
        self.run_parallel_import(date_batches, metric_type_id, start_date, end_date)
        end_time = datetime.now()

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("âœ… å¯¼å…¥å®Œæˆ:")
        print(f"  æˆåŠŸ: {len(self.progress.processed_dates)}/{self.progress.total_dates}")
        print(f"  å¤±è´¥: {len(self.progress.failed_dates)}")
        print(f"  è€—æ—¶: {end_time - start_time}")

        if self.progress.failed_dates:
            print("\nâŒ å¤±è´¥çš„æ—¥æœŸ:")
            for date in sorted(self.progress.failed_dates)[:10]:
                print(f"    - {date}")
            if len(self.progress.failed_dates) > 10:
                print(f"    ... è¿˜æœ‰ {len(self.progress.failed_dates) - 10} ä¸ª")

        print("="*60 + "\n")

        # æ¸…ç†è¿›åº¦æ–‡ä»¶ï¼ˆå¦‚æœå…¨éƒ¨å®Œæˆï¼‰
        if len(self.progress.processed_dates) == self.progress.total_dates:
            if os.path.exists(self.progress_file):
                os.remove(self.progress_file)
                print("âœ¨ è¿›åº¦æ–‡ä»¶å·²æ¸…ç†")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡å¯¼å…¥TXTæ–‡ä»¶ - æ”¯æŒå¤§æ–‡ä»¶è‡ªåŠ¨æ‹†åˆ†å’Œå¹¶è¡Œå¤„ç†'
    )

    parser.add_argument('file', help='è¦å¯¼å…¥çš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--type', choices=['TXT'], default='TXT', help='æ–‡ä»¶ç±»å‹')
    parser.add_argument('--metric-code', required=True, help='æŒ‡æ ‡ä»£ç ï¼ˆå¦‚EEEï¼‰')
    parser.add_argument('--parallel', type=int, default=4, help='å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰')
    parser.add_argument('--resume', action='store_true', help='ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­')
    parser.add_argument('--start-date', help='å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰')
    parser.add_argument('--end-date', help='ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰')

    args = parser.parse_args()

    # éªŒè¯æ–‡ä»¶
    if not os.path.exists(args.file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        sys.exit(1)

    # åˆ›å»ºå¯¼å…¥å™¨
    importer = BatchImporter(
        metric_code=args.metric_code,
        parallel=args.parallel
    )

    try:
        # æ‰§è¡Œå¯¼å…¥
        importer.import_file(
            file_path=args.file,
            resume=args.resume,
            start_date=args.start_date,
            end_date=args.end_date
        )
    except KeyboardInterrupt:
        print("\nâš ï¸ å¯¼å…¥è¢«ä¸­æ–­")
    except Exception as e:
        logger.error(f"å¯¼å…¥å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)
    finally:
        importer.cleanup()


if __name__ == '__main__':
    main()